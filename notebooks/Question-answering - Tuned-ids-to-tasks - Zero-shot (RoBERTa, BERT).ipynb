{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import coronanlp as corona\n",
    "from coronanlp.engine import ScibertQuestionAnswering\n",
    "from coronanlp.ukplab.sentence import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCES_DB = 'dev-data/543820_3858_ml15_select.pkl'\n",
    "INDEX_IVF_DB = 'dev-data/gold_index_ivg.index'\n",
    "NLI_ENCODER = '/home/ego/huggingface-models/bundles/CordBERTa/nli_stsb/0_Transformer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summarizer_model_device': device(type='cuda'),\n",
       " 'sentence_transformer_model_device': device(type='cuda'),\n",
       " 'question_answering_model_device': device(type='cpu')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = ScibertQuestionAnswering(\n",
    "    papers=corona.SentenceStore.from_disk(SENTENCES_DB),\n",
    "    encoder=SentenceTransformer(NLI_ENCODER),\n",
    "    index=faiss.read_index(INDEX_IVF_DB),\n",
    ")\n",
    "qa.all_model_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(id: 1, question: What do we know details diagnostics and surveillance?),\n",
       " Task(id: 2, question: What has been published details information sharing and inter-sectoral collaboration?),\n",
       " Task(id: 3, question: What has been published details ethical and social science considerations?),\n",
       " Task(id: 4, question: What do we know details the effectiveness of non-pharmaceutical interventions?),\n",
       " Task(id: 5, question: What has been published details medical care?),\n",
       " Task(id: 6, question: What do we know details virus genetics, origin, and evolution?),\n",
       " Task(id: 7, question: What do we know details vaccines and therapeutics?),\n",
       " Task(id: 8, question: What do we know details COVID-19 risk factors?),\n",
       " Task(id: 9, question: What is known details transmission, incubation, and environmental stability?)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasklist = corona.TaskList()\n",
    "tasklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\n"
     ]
    }
   ],
   "source": [
    "t1 = tasklist[0]\n",
    "allt1 = t1.all()\n",
    "print(t1.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ego/anaconda3/envs/ego/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1324: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[179943,  38779,  48340, 171641,  11026,  16090, 132451,  10551,\n",
       "         231547,  32627, 203359, 123822, 231157,  59945, 333167, 203328,\n",
       "          37302,  74584,   1534, 425932, 261597, 268659, 397260,  27072,\n",
       "         117127]]),\n",
       " array([[114.0309 , 119.05531, 127.31623, 128.10754, 134.01633, 135.51642,\n",
       "         137.91711, 138.64822, 139.26003, 141.16678, 142.14966, 144.36464,\n",
       "         147.06775, 147.06958, 147.22888, 148.9502 , 149.55885, 149.9512 ,\n",
       "         150.12485, 150.1834 , 150.33147, 151.04956, 151.89029, 152.3189 ,\n",
       "         152.47305]], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = qa.answer(t1.info, topk=5, top_p=25, nprobe=64, mode='bert')\n",
    "preds.popempty()\n",
    "preds.ids, preds.dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelOutput(score=0.02010919339954853, start=11, end=35, answer='laboratory confirmation,'),\n",
       " ModelOutput(score=0.00906957034021616, start=133, end=143, answer='sequencing.'),\n",
       " ModelOutput(score=0.00279330019839108, start=40, end=143, answer='national reference laboratory aims to obtain material from regional laboratories for further sequencing.'),\n",
       " ModelOutput(score=0.002625082153826952, start=11, end=21, answer='laboratory')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11, 35), (133, 143), (40, 143), (11, 21)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.spans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national reference laboratory aims to obtain material from regional laboratories for further sequencing.\n"
     ]
    }
   ],
   "source": [
    "out = preds[2]\n",
    "print(out.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\n",
      "\n",
      "Answer: national reference laboratory aims to obtain material from regional laboratories for further sequencing.\n",
      "\n",
      "Context:\n",
      "\n",
      "In case of laboratory confirmation, the << national reference laboratory aims to obtain material from regional laboratories for further sequencing. [ANSWER] >> An alternate measure of program success is the extent to which screening delays the first importation of cases into the community, possibly providing additional time to train medical staff, deploy public health responders or refine travel policies (Cowling et al., Unlike the UK national strategy documents and plans, the US National Health Information Infrastructure Strategy document (also known as \"Information for Health\") refers explicitly to GIS and real-time health and disease monitoring and states that \"public health will need to include in its toolkit integrated data systems; high-quality community-level data; tools to identify significant health trends in real-time data streams; and geographic information systems\" [48]. This thinktank is used to address matters related to combating new issues and to expand on the Saudi reach to the international scientific community. For the purpose of this paper, the following de�nition is used, \"Public health surveillance is the ongoing systematic collection, analysis, interpretation and dissemination of health data for the planning, implementation and evaluation of public health action\" (see Section 2.3 below). Finally, we discuss policy and logistical and technological obstacles to achieving a potential transformation of public health microbiology.\n"
     ]
    }
   ],
   "source": [
    "question, context, answer = preds.q, ' '.join(preds.c), out.answer\n",
    "corona.render_output(answer=answer, context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visual tools to assess the plausibility of algorithm- identified infectious disease clusters: an application to mumps data from the netherlands',\n",
       " 'in silico approach to accelerate the development of mass spectrometry-based proteomics methods for detection of viral proteins: application to covid-19',\n",
       " 'towards evidence-based, gis-driven national spatial health information infrastructure and surveillance services in the united kingdom',\n",
       " 'hajj, umrah, and the neglected tropical diseases',\n",
       " 'the past, present, and future of public health surveillance',\n",
       " 'improved global capacity for influenza surveillance sign up for twitter and find the latest information about emerging infectious diseases from the eid journal. @cdc_eidjournal',\n",
       " 'epidemiologic data and pathogen genome sequences: a powerful synergy for public health',\n",
       " \"strengthening field-based training in low and middle-income countries to build public health capacity: lessons from australia's master of applied epidemiology program\",\n",
       " 'strengthening timely detection and reporting of unusual respiratory events from health facilities in yaoundé, cameroon',\n",
       " 'systematic reviews lessons learnt from implementation of the international health regulations: a systematic review',\n",
       " 'tropical medicine and infectious disease policy and science for global health security: shaping the course of international health',\n",
       " 'metagenomics for pathogen detection in public health']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sids = preds.ids.tolist()[0]\n",
    "pids = list(qa.papers.lookup(sids, mode='table').keys())\n",
    "titles = [x.lower() for x in qa.cord19.titles(pids)]\n",
    "titles[::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With growing amounts of data available, identification of clusters of persons linked to each other by transmission of an infectious disease increasingly relies on automated algorithms. We propose cluster finding to be a two-step process: first, possible transmission clusters are identified using a cluster algorithm, second, the plausibility that the identified clusters represent genuine transmission clusters is evaluated. Aim: To introduce visual tools to assess automatically identified clusters. Methods: We developed tools to visualise: (i) clusters found in dimensions of time, geographical location and genetic data; (ii) nested sub-clusters within identified clusters; (iii) intra-cluster pairwise dissimilarities per dimension; (iv) intra-cluster correlation between dimensions. We applied our tools to notified mumps cases in the Netherlands with available disease onset date (January 2009 -June 2016), geographical information (location of residence), and pathogen sequence data (n = 112). We compared identified clusters to clusters reported by the Netherlands Early Warning Committee (NEWC). Results: We identified five mumps clusters. Three clusters were considered plausible. One was questionable because, in phylogenetic analysis, genetic sequences related to it segregated in two groups. One was implausible with no smaller nested clusters, high intra-cluster dissimilarities on all dimensions, and low intra-cluster correlation between dimensions. The NEWC reports concurred with our findings: the plausible/questionable clusters corresponded to reported outbreaks; the implausible cluster did not. Conclusion: Our tools for assessing automatically identified clusters allow outbreak investigators to rapidly spot plausible transmission clusters for mumps and other human-to-human transmissible diseases. This fast information processing potentially reduces workload.\n"
     ]
    }
   ],
   "source": [
    "abstract = qa.cord19.load_paper(pids[0])['abstract']\n",
    "print(abstract[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('health', 1.0),\n",
       " ('public', 0.5625),\n",
       " ('disease', 0.375),\n",
       " ('surveillance', 0.375),\n",
       " ('infectious', 0.3125),\n",
       " ('detection', 0.25),\n",
       " ('diseases', 0.1875),\n",
       " ('review', 0.1875),\n",
       " ('epidemiology', 0.1875),\n",
       " ('global', 0.1875)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = corona.common_tokens(titles, nlp=qa.nlp)\n",
    "labels, freqs = zip(*pred_labels)\n",
    "labelmap = dict(zip(labels, [k/max(freqs) for k in freqs]))\n",
    "list(labelmap.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zero-shot-classification\n",
    "\n",
    "Let's test the predicted answers from the question answering model finetuned on SQUAD 2.0 with `CordBERTa`.\n",
    "\n",
    "- **CordBERTa**\n",
    "\n",
    "    - base-model: roberta-base-cased\n",
    "\n",
    "    - fine-tuned: language-modeling\n",
    "\n",
    "    - data/tokens: CORD19 kaggle dataset.\n",
    "    \n",
    "    - downstream-task: MNLI (sequence classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "CORDBERTA = '/home/ego/huggingface-models/finetuned/roberta_mnli_cord19/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\n",
    "    task='zero-shot-classification',\n",
    "    model=RobertaForSequenceClassification.from_pretrained(CORDBERTA),\n",
    "    tokenizer=RobertaTokenizer.from_pretrained(CORDBERTA),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'laboratory confirmation,',\n",
       "  'labels': ['surveillance', 'disease', 'health', 'public', 'infectious'],\n",
       "  'scores': [0.8520071506500244,\n",
       "   0.745579183101654,\n",
       "   0.6930623054504395,\n",
       "   0.6136701703071594,\n",
       "   0.5498717427253723]},\n",
       " {'sequence': 'sequencing.',\n",
       "  'labels': ['surveillance', 'health', 'disease', 'infectious', 'public'],\n",
       "  'scores': [0.8411460518836975,\n",
       "   0.6569487452507019,\n",
       "   0.6352137923240662,\n",
       "   0.5501412749290466,\n",
       "   0.5465444922447205]},\n",
       " {'sequence': 'national reference laboratory aims to obtain material from regional laboratories for further sequencing.',\n",
       "  'labels': ['surveillance', 'health', 'disease', 'public', 'infectious'],\n",
       "  'scores': [0.8664122819900513,\n",
       "   0.5979554653167725,\n",
       "   0.40629252791404724,\n",
       "   0.37213629484176636,\n",
       "   0.22388437390327454]},\n",
       " {'sequence': 'laboratory',\n",
       "  'labels': ['surveillance', 'disease', 'health', 'public', 'infectious'],\n",
       "  'scores': [0.7985999584197998,\n",
       "   0.7546665668487549,\n",
       "   0.752849280834198,\n",
       "   0.7494865655899048,\n",
       "   0.731054961681366]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis_template = 'This text is about {}.'\n",
    "sequences = [o.answer for o in preds]\n",
    "candidate_labels = list(labels[:5])\n",
    "\n",
    "zeroshot = classifier(sequences, candidate_labels,\n",
    "                      hypothesis_template, multi_class=True)\n",
    "zeroshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above, we can see that the classifier scores the label `<surveillance>` as the closest match for all predicted answers/sequences (from the question-answering model). Considering we did not directly use the titles as inputs, `<surveillance>` is indeed related to the context of the question asked:\n",
    "\n",
    "**\"What has been published concerning the systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?\".**\n",
    "\n",
    "> Keep in mind; the models did not have access to the titles/labels. Instead, we \"forced\" the models to put together a \"report/summary\" using `3,858` research papers or, to be more exact, `543,820` sentences. Another fact to consider; the papers/paper-ids were picked or filtered from a total of `13,202` possible papers to choose from by using the method `Tune IDs to Tasks.` The way it works is simple; Given some `tasks` and a `title_map` (a mapping between `{paper-ids: titles}` e.g., **{4514: \"visual tools to assess the plausibility of algorithm ...\"}**) where the `tasks` are expressed as queries and `titles` as the database - both represented as dense vectors. We compute each query's association against the database and then sort by the weighted distribution of scored ids. In other words, from the most commonly accessed by each query to the least accessed by each query.\n",
    "\n",
    "```python\n",
    "from coronanlp import tune_ids, extract_titles_fast\n",
    "from coronanlp import TaskList, CORD19\n",
    "from coronanlp.ukplab import SentenceTransformer\n",
    "\n",
    "cord19 = CORD19(...)\n",
    "print(cord19)\n",
    "```\n",
    "\n",
    "* The results in this notebook are from the following sources:\n",
    "\n",
    "```\n",
    "CORD19(papers: 13202, files_sorted: True, source: [\n",
    "  biorxiv_medrxiv, noncomm_use_subset, comm_use_subset, pmc_custom_license,\n",
    "])\n",
    "```\n",
    "\n",
    "* Here are the steps to obtain the results metioned above:\n",
    "\n",
    "```python\n",
    "...\n",
    "title_map = extract_titles_fast(cord19, minlen=10, maxids=-1)\n",
    "gold_ids = tune_ids(\n",
    "    encoder=SentenceTransformer(...),\n",
    "    title_map=title_map,\n",
    "    task_list=TaskList(), \n",
    "    target_size=1800,\n",
    ")\n",
    "# If you already have an instance of SentenceStore e.g if is the full dataset.\n",
    "gold_papers = papers.index_select(gold_ids)\n",
    "\n",
    "# If not you can simply do:\n",
    "gold_sample = list(gold_ids.sample())\n",
    "gold_papers = cord19.batch(gold_sample, minlen=15)\n",
    "\n",
    "print(gold_papers)\n",
    "# SentenceStore(avg_seqlen=180.96, num_papers=3858, num_sents=543820)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: max_length: 128, device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "encoder_model = qa.encoder._modules['0'].model\n",
    "encoder_tokenizer = qa.encoder.tokenizer\n",
    "encoder_maxlength = qa.encoder.max_length\n",
    "encoder_device = encoder_model.device\n",
    "print('Encoder: max_length: {}, device: {}'.format(\n",
    "    encoder_maxlength, encoder_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 768]), torch.Size([5, 768]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pairs = [zeroshot[2]['sequence']] + zeroshot[2]['labels']\n",
    "\n",
    "inputs = encoder_tokenizer.batch_encode_plus(\n",
    "    text_pairs,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=encoder_maxlength,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "inputs = inputs.to(encoder_device)\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "output = encoder_model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "sentence_rep = output[:1].mean(dim=1)  # == text_pairs[:1]\n",
    "label_reps = output[1:].mean(dim=1)   # == text_pairs[1:]\n",
    "sentence_rep.shape, label_reps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['national reference laboratory aims to obtain material from regional laboratories for further sequencing.'],\n",
       " ['surveillance', 'health', 'disease', 'public', 'infectious'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pairs[:1], text_pairs[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<k: -0.18992996215820312>\tcosine | zeroshot:\tpublic | surveillance\n",
      "<k: -0.2037743777036667>\tcosine | zeroshot:\tsurveillance | health\n",
      "<k: -0.2134021520614624>\tcosine | zeroshot:\thealth | disease\n",
      "<k: -0.21375295519828796>\tcosine | zeroshot:\tdisease | public\n",
      "<k: -0.24101680517196655>\tcosine | zeroshot:\tinfectious | infectious\n"
     ]
    }
   ],
   "source": [
    "similarities = F.cosine_similarity(sentence_rep, label_reps, dim=1)\n",
    "closest = similarities.argsort(descending=True)\n",
    "\n",
    "for zero_idx, cosine_idx in enumerate(closest):\n",
    "    print('<k: {1}>\\tcosine | zeroshot:\\t{0} | {2}'.format(\n",
    "        text_pairs[1:][cosine_idx], similarities[cosine_idx],\n",
    "        text_pairs[1:][zero_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
